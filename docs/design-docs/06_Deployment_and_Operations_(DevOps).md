# ë°°í¬ ë° ìš´ì˜ (Deployment and Operations - DevOps)

## ğŸ“‹ ë¬¸ì„œ ê°œìš”

**ë¬¸ì„œ ëª©ì **: ê°œë°œëœ ìë™ ì•”í˜¸í™”í ê±°ë˜ ì‹œìŠ¤í…œì„ Google Cloud Platform(GCP)ì— ë°°í¬í•˜ê³  ê´€ë¦¬í•˜ê¸° ìœ„í•œ ê·œë²”ì ì´ê³  í¬ê´„ì ì¸ ê°€ì´ë“œ

**í•µì‹¬ ì² í•™**: 
- ğŸ¤– **ìë™í™”**: ì½”ë“œ í‘¸ì‹œë¶€í„° ì„œë¹„ìŠ¤ ì¬ì‹œì‘ê¹Œì§€ì˜ ì „ ê³¼ì • ìë™í™”
- ğŸ“ **ì½”ë“œí˜• ì¸í”„ë¼(IaC)**: ëª¨ë“  ì¸í”„ë¼ êµ¬ì„±ì„ ì½”ë“œë¡œ ê´€ë¦¬
- ğŸ›¡ï¸ **ë‹¤ì¸µì  ë³´ì•ˆ**: ì¸í”„ë¼ì™€ ì• í”Œë¦¬ì¼€ì´ì…˜ ì „ë°˜ì˜ í†µí•© ë³´ì•ˆ
- ğŸ” **ì§€ëŠ¥í˜• ëª¨ë‹ˆí„°ë§**: AI ê¸°ë°˜ ì´ìƒ ê°ì§€ ë° ì˜ˆì¸¡ì  ì•Œë¦¼
- ğŸ”„ **ì¬í•´ ë³µêµ¬**: ë¬´ì¤‘ë‹¨ ì„œë¹„ìŠ¤ë¥¼ ìœ„í•œ í¬ê´„ì  DR ì „ëµ

**ëª©í‘œ**: ìˆ˜ë™ ê°œì…ìœ¼ë¡œ ì¸í•œ ì˜¤ë¥˜ ìµœì†Œí™”, ë°˜ë³µ ê°€ëŠ¥ì„±ê³¼ ê°ì‚¬ ìš©ì´ì„± ë³´ì¥

**ëŒ€ìƒ ë…ì**: DevOps ì—”ì§€ë‹ˆì–´, ì‹œìŠ¤í…œ ê´€ë¦¬ì, SRE íŒ€

---

## ğŸ—ï¸ 1. ì½”ë“œí˜• ì¸í”„ë¼ (Infrastructure as Code)

### 1.1 í•µì‹¬ ì›ì¹™

**ëª¨ë“  í´ë¼ìš°ë“œ ì¸í”„ë¼ëŠ” ì½”ë“œë¡œ ì •ì˜ë˜ê³  ê´€ë¦¬**: ë°˜ë³µ ê°€ëŠ¥í•˜ê³ , ë²„ì „ ì œì–´ê°€ ê°€ëŠ¥í•˜ë©°, ê°ì‚¬ê°€ ìš©ì´í•œ í™˜ê²½ ë³´ì¥

**ê°•ë ¥í•œ ê¸ˆì§€ì‚¬í•­**: 
- âŒ **í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ GCP ì½˜ì†”ì„ í†µí•œ ìˆ˜ë™ ì„¤ì • ì ˆëŒ€ ê¸ˆì§€**
- âš ï¸ **ì´ìœ **: ì˜¤ë¥˜ ìœ ë°œ ê°€ëŠ¥ì„±ê³¼ ì¶”ì  ë¶ˆê°€ëŠ¥ì„±

### 1.2 ì‚¬ìš© ë„êµ¬

**ê¶Œì¥ ë„êµ¬**: 
- âœ… **Terraform** - ì„ í˜¸í•˜ëŠ” IaC ë„êµ¬
- âœ… **gcloud CLI ìŠ¤í¬ë¦½íŠ¸** - ëŒ€ì•ˆ ë„êµ¬

### 1.3 í”„ë¡œë¹„ì €ë‹ ëŒ€ìƒ ë¦¬ì†ŒìŠ¤

**IaC ìŠ¤í¬ë¦½íŠ¸ê°€ ì •ì˜í•˜ê³  ìƒì„±í•´ì•¼ í•˜ëŠ” ëª¨ë“  GCP ë¦¬ì†ŒìŠ¤**:

#### ğŸŒ **VPC ë„¤íŠ¸ì›Œí¬**
- **ì‚¬ìš©ì ì •ì˜ VPC** ë„¤íŠ¸ì›Œí¬ ë° ì„œë¸Œë„·
- **ë°©í™”ë²½ ê·œì¹™**:
  - íŠ¹ì • IPì—ì„œì˜ SSH í—ˆìš©
  - ê±°ë˜ì†Œ APIë¡œì˜ Egress íŠ¸ë˜í”½ í—ˆìš©

#### ğŸ’» **ì»´í“¨íŒ…**
- **Google Compute Engine (GCE)** VM ì¸ìŠ¤í„´ìŠ¤

#### ğŸ—„ï¸ **ë°ì´í„°ë² ì´ìŠ¤**
- **Cloud SQL (PostgreSQL)** ì¸ìŠ¤í„´ìŠ¤
- **ë¹„ê³µê°œ IP êµ¬ì„±** í¬í•¨

#### ğŸ”§ **CI/CD ë° ì•„í‹°íŒ©íŠ¸**
- **Artifact Registry ì €ì¥ì†Œ** (Docker ì´ë¯¸ì§€ ì €ì¥ìš©)

#### ğŸ” **ë³´ì•ˆ ë° ì‹ ì›**
- **GCP Secret Manager**ì˜ ëª¨ë“  ë¹„ë°€(secret)ë“¤
- **ì „ìš© IAM ì„œë¹„ìŠ¤ ê³„ì •** ë° ì—­í•  ë°”ì¸ë”© (ìµœì†Œ ê¶Œí•œ ì›ì¹™ ì ìš©)

---

## ğŸš€ 2. CI/CD íŒŒì´í”„ë¼ì¸: Git Pushì—ì„œ ì‹¤ì‹œê°„ ë°°í¬ê¹Œì§€

### 2.1 ëª©í‘œ

**ì™„ì „ ìë™í™”**: ìƒˆë¡œìš´ ì½”ë“œë¥¼ ê°œë°œìì˜ ë¡œì»¬ ë¨¸ì‹ ì—ì„œ í”„ë¡œë•ì…˜ í™˜ê²½ì˜ ì‹¤í–‰ ì¤‘ì¸ ì„œë¹„ìŠ¤ë¡œ **ìˆ˜ë™ ê°œì… ì—†ì´** ì•ˆì „í•˜ê²Œ ë°°í¬

### 2.2 íŠ¸ë¦¬ê±° ì„¤ì •

**ìë™ ì‹¤í–‰ ì¡°ê±´**: Git ì €ì¥ì†Œì˜ `main` ë¸Œëœì¹˜ì— ìƒˆë¡œìš´ ì»¤ë°‹ì´ í‘¸ì‹œë  ë•Œë§ˆë‹¤ Cloud Build íŠ¸ë¦¬ê±° ìë™ ì‹œì‘

### 2.3 íŒŒì´í”„ë¼ì¸ 4ë‹¨ê³„ êµ¬ì„±

**ëª¨ë“  ë‹¨ê³„ëŠ” `cloudbuild.yaml` íŒŒì¼ì— ëª…ì‹œì ìœ¼ë¡œ ì •ì˜**:

#### **1ë‹¨ê³„: í…ŒìŠ¤íŠ¸ (Test)**
- âœ… **ë¦°í„°(linter)** ì‹¤í–‰ìœ¼ë¡œ ì½”ë“œ ìŠ¤íƒ€ì¼ ê²€ì‚¬
- âœ… **ëª¨ë“  ë‹¨ìœ„ ë° í†µí•© í…ŒìŠ¤íŠ¸** ì‹¤í–‰ìœ¼ë¡œ ì½”ë“œ ì •í™•ì„± ê²€ì¦

#### **2ë‹¨ê³„: ë¹Œë“œ (Build)**
- âœ… **ë‹¤ë‹¨ê³„ ë¹Œë“œ(multi-stage build)** ì‚¬ìš©í•˜ëŠ” Dockerfile
- âœ… **ê²½ëŸ‰í™”ë˜ê³  ìµœì í™”ëœ** Docker ì´ë¯¸ì§€ ë¹Œë“œ

#### **3ë‹¨ê³„: í‘¸ì‹œ (Push)**
- âœ… **Git ì»¤ë°‹ í•´ì‹œ($COMMIT_SHA)** íƒœê·¸ ì§€ì •
- âœ… **Artifact Registry ì €ì¥ì†Œ**ì— ì´ë¯¸ì§€ í‘¸ì‹œ
- ğŸ¯ **íš¨ê³¼**: ëª¨ë“  ë¹Œë“œë¥¼ ê³ ìœ í•˜ê²Œ ì‹ë³„í•˜ê³  ì¶”ì  ê°€ëŠ¥

#### **4ë‹¨ê³„: ë°°í¬ (Deploy)**
- âœ… **gcloud compute ssh** ëª…ë ¹ì–´ë¡œ ëŒ€ìƒ GCE VMì— ì›ê²© ì ‘ì†
- âœ… **ë°°í¬ ìŠ¤í¬ë¦½íŠ¸** ì‹¤í–‰:
  - Artifact Registryì—ì„œ ìƒˆ ì´ë¯¸ì§€ ë²„ì „ ê°€ì ¸ì˜¤ê¸° (`docker pull`)
  - í•´ë‹¹ systemd ì„œë¹„ìŠ¤ ì¬ì‹œì‘ (`sudo systemctl restart <service-name>`)
- ğŸš€ **ì¥ì **: VM ì „ì²´ ì¬ë¶€íŒ…ë³´ë‹¤ í›¨ì”¬ ë¹ ë¥´ê³  íš¨ìœ¨ì 

### 2.4 cloudbuild.yaml êµ¬ì„± ì˜ˆì‹œ

**íŠ¹ì • ì„œë¹„ìŠ¤(core-engine) ë¹Œë“œ ë° ë°°í¬ ì™„ì „í•œ ì˜ˆì‹œ**:

```yaml
# cloudbuild.yaml
# ì´ íŒŒì´í”„ë¼ì¸ì€ Docker ì´ë¯¸ì§€ë¥¼ ë¹Œë“œí•˜ê³ , Artifact Registryì— í‘¸ì‹œí•œ í›„,
# GCE VMì˜ ì»¨í…Œì´ë„ˆë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
steps:
  # 1. íŠ¹ì • ì„œë¹„ìŠ¤(ì˜ˆ: core-engine)ìš© Docker ì´ë¯¸ì§€ ë¹Œë“œ
  - name: 'gcr.io/cloud-builders/docker'
    args:
      - 'build'
      - '-t'
      - '${_LOCATION}-docker.pkg.dev/${PROJECT_ID}/${_REPO_NAME}/${_SERVICE_NAME}:$COMMIT_SHA'
      - './app/${_SERVICE_NAME}' # ê° ì„œë¹„ìŠ¤ëŠ” ìì²´ í•˜ìœ„ ë””ë ‰í† ë¦¬ì— Dockerfileì„ ê°€ì§€ê³  ìˆë‹¤ê³  ê°€ì •
    id: 'Build ${_SERVICE_NAME}'

  # 2. Artifact Registryì— ì´ë¯¸ì§€ í‘¸ì‹œ
  - name: 'gcr.io/cloud-builders/docker'
    args:
      - 'push'
      - '${_LOCATION}-docker.pkg.dev/${PROJECT_ID}/${_REPO_NAME}/${_SERVICE_NAME}:$COMMIT_SHA'
    id: 'Push ${_SERVICE_NAME}'

  # 3. GCEì— ë°°í¬: VMì—ì„œ ì›ê²©ìœ¼ë¡œ ë°°í¬ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
  # ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ìƒˆ ì´ë¯¸ì§€ë¥¼ ê°€ì ¸ì™€ systemd ì„œë¹„ìŠ¤ë¥¼ ì¬ì‹œì‘í•©ë‹ˆë‹¤.
  - name: 'gcr.io/google.com/cloudsdktool/google-cloud-cli'
    entrypoint: 'gcloud'
    args:
      - 'compute'
      - 'ssh'
      - '${_INSTANCE_NAME}'
      - '--zone=${_ZONE}'
      - '--command="sudo /opt/scripts/deploy.sh ${_SERVICE_NAME}"'
    id: 'Deploy ${_SERVICE_NAME}'

# ë¹Œë“œ ë° í‘¸ì‹œí•  ì´ë¯¸ì§€ ì§€ì •
images:
  - '${_LOCATION}-docker.pkg.dev/${PROJECT_ID}/${_REPO_NAME}/${_SERVICE_NAME}:$COMMIT_SHA'

# ëŒ€ì²´ ë³€ìˆ˜. Cloud Build íŠ¸ë¦¬ê±°ì—ì„œ ì„¤ì • ê°€ëŠ¥
substitutions:
  _LOCATION: 'us-central1'
  _REPO_NAME: 'trading-bot-repo'
  _SERVICE_NAME: 'core-engine' # ì‹¤ì œ íŠ¸ë¦¬ê±°ì—ì„œëŠ” ì´ ê°’ì„ íŒŒë¼ë¯¸í„°í™”í•  ìˆ˜ ìˆìŒ
  _INSTANCE_NAME: 'trading-bot-vm'
  _ZONE: 'us-central1-a'
```

---

## ğŸ³ 3. ëŸ°íƒ€ì„ í™˜ê²½

### 3.1 ì»¨í…Œì´ë„ˆí™”: ë‹¤ë‹¨ê³„ Dockerfile

#### ğŸ¯ **ë‹¤ë‹¨ê³„ ë¹Œë“œ ëª©ì **
- **ì²« ë²ˆì§¸ ë‹¨ê³„**: ë¹Œë“œì— í•„ìš”í•œ ëª¨ë“  ë„êµ¬ì™€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜, ì¢…ì†ì„± ì»´íŒŒì¼
- **ë‘ ë²ˆì§¸ ë‹¨ê³„**: ì‹¤ì œ ì‹¤í–‰ì— í•„ìš”í•œ ìµœì†Œí•œì˜ íŒŒì¼ê³¼ ë¼ì´ë¸ŒëŸ¬ë¦¬ë§Œ ë³µì‚¬
- **íš¨ê³¼**: ìµœì¢… ì´ë¯¸ì§€ í¬ê¸° ì¤„ì´ê³  ë³´ì•ˆ ê°•í™”

#### ğŸ’» **Dockerfile ì˜ˆì‹œ**

```dockerfile
# Dockerfile for a Python service

# --- Build Stage ---
# Use a full Python image to install dependencies, including build tools
FROM python:3.11 as builder
WORKDIR /usr/src/app

# Install build-time system dependencies if needed (e.g., for psycopg2)
# RUN apt-get update && apt-get install -y --no-install-recommends build-essential libpq-dev

# Copy requirements and install them into a wheelhouse
COPY requirements.txt ./
RUN pip wheel --no-cache-dir --wheel-dir /usr/src/app/wheels -r requirements.txt

# --- Production Stage ---
# Use a slim image for the final container to reduce size and attack surface
FROM python:3.11-slim
WORKDIR /usr/src/app

# Install runtime system dependencies if needed (e.g., for psycopg2)
# RUN apt-get update && apt-get install -y --no-install-recommends libpq5 && rm -rf /var/lib/apt/lists/*

# Copy pre-built wheels from the builder stage and install them
COPY --from=builder /usr/src/app/wheels /wheels
RUN pip install --no-cache /wheels/*

# Copy the application source code
COPY . .

# Run the application. The -u flag ensures that logs are not buffered.
CMD ["python", "-u", "main.py"]
```

### 3.2 ì„œë¹„ìŠ¤ ê´€ë¦¬: GCEì—ì„œ systemd ì‚¬ìš©

#### ğŸ›¡ï¸ **systemd ì„ íƒì˜ ì •ë‹¹ì„±**

**ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì‚¬í•­**:
- ğŸ”„ **24/7 ì¤‘ë‹¨ ì—†ëŠ” ì‹¤í–‰** í•„ìš”
- ğŸ”§ **ìë™ ë³µêµ¬**: ì• í”Œë¦¬ì¼€ì´ì…˜ ì¶©ëŒì´ë‚˜ VM ì¬ë¶€íŒ… ì‹œ ìë™ ë³µêµ¬

**ê¸°ìˆ ì  ê·¼ê±°**:
- âŒ **Docker í•œê³„**: ì»¨í…Œì´ë„ˆí™”ëŠ” ì œê³µí•˜ì§€ë§Œ í”„ë¡œì„¸ìŠ¤ ê´€ë¦¬ëŠ” ë¯¸ì œê³µ
- âœ… **systemd ì¥ì **: ìµœì‹  ë¦¬ëˆ…ìŠ¤ ë°°í¬íŒì˜ í‘œì¤€ì ì´ê³  ë§¤ìš° ê²¬ê³ í•œ ì„œë¹„ìŠ¤ ê´€ë¦¬ì

#### ğŸ“‹ **ìœ ë‹› íŒŒì¼ êµ¬ì„±**
**ìœ„ì¹˜**: `/etc/systemd/system/` ë””ë ‰í† ë¦¬
**íŒŒì¼ëª… ì˜ˆì‹œ**: `trading-bot-core.service`

#### ğŸ’» **ìœ ë‹› íŒŒì¼ ì˜ˆì‹œ**

```ini
[Unit]
Description=Trading Bot Core Engine
After=docker.service
Requires=docker.service

[Service]
TimeoutStartSec=0
Restart=always
RestartSec=10s
ExecStartPre=-/usr/bin/docker kill core-engine
ExecStartPre=-/usr/bin/docker rm core-engine
ExecStartPre=/usr/bin/docker pull us-central1-docker.pkg.dev/your-project/your-repo/core-engine:latest
ExecStart=/usr/bin/docker run --name core-engine --rm --network=host us-central1-docker.pkg.dev/your-project/your-repo/core-engine:latest
ExecStop=-/usr/bin/docker stop core-engine

[Install]
WantedBy=multi-user.target
```

#### ğŸ”§ **ì£¼ìš” ì„¤ì • ì˜µì…˜ ì„¤ëª…**

| ì„¤ì • ì˜µì…˜ | ê¸°ëŠ¥ | íš¨ê³¼ |
|-----------|------|------|
| `Restart=always` | ìë™ ì¬ì‹œì‘ | ì„œë¹„ìŠ¤ê°€ ì–´ë–¤ ì´ìœ ë¡œë“  ì‹¤íŒ¨í•˜ë©´ systemdê°€ í•­ìƒ ì¬ì‹œì‘ ë³´ì¥ |
| `ExecStartPre` | ì‚¬ì „ ì‹¤í–‰ ëª…ë ¹ | ì„œë¹„ìŠ¤ ì‹œì‘ ì „ ì´ì „ ì»¨í…Œì´ë„ˆ ì •ë¦¬ ë° ìµœì‹  ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸° |
| `--network=host` | ë„¤íŠ¸ì›Œí¬ ì„¤ì • | ì»¨í…Œì´ë„ˆê°€ VMì˜ ë„¤íŠ¸ì›Œí¬ ìŠ¤íƒì„ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ Cloud SQL í†µì‹  ìš©ì´ |

**ì‹œë„ˆì§€ íš¨ê³¼**: systemdì™€ Cloud Buildì˜ ì¡°í•©ìœ¼ë¡œ **ê²¬ê³ í•˜ê³  ìë™í™”ëœ ë°°í¬ ì›Œí¬í”Œë¡œìš°** ì™„ì„±

---

## ğŸ›¡ï¸ 4. í¬ê´„ì ì¸ ë³´ì•ˆ íƒœì„¸

### 4.1 ë³´ì•ˆ ì² í•™

**ë‹¤ì¸µì  ì ‘ê·¼**: ì‹œìŠ¤í…œì˜ ë³´ì•ˆì€ ë‹¨ì¼ ê¸°ëŠ¥ì´ ì•„ë‹ˆë¼, **ì¸í”„ë¼ì™€ ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ê³„ ì „ë°˜ì— ê±¸ì³ í†µí•©ëœ ë‹¤ì¸µì  ì ‘ê·¼ ë°©ì‹**ì„ í†µí•´ ë‹¬ì„±

### 4.2 ì‹ ì› ë° ì ‘ê·¼ ê´€ë¦¬ (IAM)

#### ğŸ‘¤ **ì „ìš© ì„œë¹„ìŠ¤ ê³„ì •**
- **ê³„ì •ëª…**: `trading-bot-sa`
- **ëª©ì **: GCE VM ì „ìš© ì„œë¹„ìŠ¤ ê³„ì • ìƒì„±

#### ğŸ” **ìµœì†Œ ê¶Œí•œ ì›ì¹™**
**í•„ìˆ˜ ì—­í• ë§Œ ë¶€ì—¬**:
- âœ… `Secret Manager Secret Accessor`
- âœ… `Cloud SQL Client`
- âœ… `Artifact Registry Reader`
- âœ… `Logs Writer`
- âœ… `Monitoring Metric Writer`

**ì ˆëŒ€ ê¸ˆì§€**:
- âŒ `Editor` ì—­í•  ë¶€ì—¬ ê¸ˆì§€
- âŒ `Owner` ì—­í•  ë¶€ì—¬ ê¸ˆì§€
- âš ï¸ **ì´ìœ **: ê´‘ë²”ìœ„í•œ ê¸°ë³¸ ì—­í• ì€ ë³´ì•ˆ ìœ„í—˜ ì¦ê°€

### 4.3 ë„¤íŠ¸ì›Œí¬ ë³´ì•ˆ (VPC)

#### ğŸŒ **VPC êµ¬ì„±**
- **ì‚¬ìš©ì ì •ì˜ VPC**: ëª¨ë“  ë¦¬ì†ŒìŠ¤ë¥¼ ê¸°ë³¸ VPCê°€ ì•„ë‹Œ ì‚¬ìš©ì ì •ì˜ VPCì— ë°°ì¹˜
- **ë¹„ê³µê°œ IP**: Cloud SQL ì¸ìŠ¤í„´ìŠ¤ëŠ” ë¹„ê³µê°œ IPë¡œë§Œ êµ¬ì„±í•˜ì—¬ ì™¸ë¶€ ì¸í„°ë„· ì ‘ê·¼ ì›ì²œ ì°¨ë‹¨

#### ğŸ”¥ **ë°©í™”ë²½ ê·œì¹™**
**ì—„ê²©í•œ êµ¬ì„± ì›ì¹™**:
- âœ… **í—ˆìš©**: íŠ¹ì • IPì—ì„œì˜ SSH
- âœ… **í—ˆìš©**: ê±°ë˜ì†Œ APIë¡œì˜ Egress íŠ¸ë˜í”½
- âŒ **ì°¨ë‹¨**: ê¸°íƒ€ ëª¨ë“  ë¶ˆí•„ìš”í•œ íŠ¸ë˜í”½

### 4.4 ë¹„ë°€ ê´€ë¦¬ (Secret Manager)

#### ğŸ¦ **ì¤‘ì•™ ì €ì¥ì†Œ (ë³´ì•ˆ ëª¨ë¸ì˜ ì´ˆì„)**
**ì €ì¥ ëŒ€ìƒ**:
- ğŸ”‘ ê±°ë˜ì†Œ API í‚¤
- ğŸ“± í…”ë ˆê·¸ë¨ ë´‡ í† í°
- ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ìê²© ì¦ëª…
- ğŸ” ê¸°íƒ€ ëª¨ë“  ë¯¼ê°í•œ ë°ì´í„°

#### ğŸ”„ **ë™ì  ë¡œë“œ**
```python
# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œì—ì„œ ëŸ°íƒ€ì„ì— ë¹„ë°€ ê°€ì ¸ì˜¤ê¸°
from google.cloud import secretmanager

client = secretmanager.SecretManagerServiceClient()
secret_value = client.access_secret_version(request={"name": secret_path}).payload.data.decode("UTF-8")
```

#### âš ï¸ **ì ˆëŒ€ ê¸ˆì§€ ì‚¬í•­**
**ë¹„ë°€ ì •ë³´ ì €ì¥ ê¸ˆì§€ ìœ„ì¹˜**:
- âŒ êµ¬ì„± íŒŒì¼
- âŒ Git ì €ì¥ì†Œ
- âŒ í™˜ê²½ ë³€ìˆ˜ íŒŒì¼
- âŒ Docker ì´ë¯¸ì§€

**ì¤‘ìš”ì„±**: ê°€ì¥ í”í•˜ë©´ì„œë„ ì¹˜ëª…ì ì¸ ë³´ì•ˆ ì‹¤ìˆ˜ ì¤‘ í•˜ë‚˜

### 4.5 ê±°ë˜ì†Œ API í‚¤ ë³´ì•ˆ

#### ğŸ›¡ï¸ **ì¶”ê°€ ë³´ì•ˆ ê³„ì¸µ**
**Secret Manager ì €ì¥ + ê±°ë˜ì†Œ ìì²´ ë³´ì•ˆ ê¸°ëŠ¥ ìµœëŒ€ í™œìš©**

#### ğŸ“ **IP í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸**
**ì„¤ì • ë°©ë²•**:
1. API í‚¤ ìƒì„± ì‹œ GCE VMì˜ **ê³ ì • ì™¸ë¶€ IP ì£¼ì†Œ**ë¥¼ IP í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ì— ë“±ë¡
2. **ë³´ì•ˆ íš¨ê³¼**: API í‚¤ê°€ ìœ ì¶œë˜ë”ë¼ë„ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ì— ë“±ë¡ë˜ì§€ ì•Šì€ IPì—ì„œëŠ” ì‚¬ìš© ë¶ˆê°€

**ì¤‘ìš”ì„±**: ë§¤ìš° ì¤‘ìš”í•œ ë°©ì–´ ê³„ì¸µìœ¼ë¡œ í”¼í•´ë¥¼ ì›ì²œì ìœ¼ë¡œ ì°¨ë‹¨

### 4.6 ê³ ê¸‰ ë³´ì•ˆ ê°•í™” (ì‹ ê·œ ì¶”ê°€)

#### ğŸ” **ì·¨ì•½ì  ìŠ¤ìºë‹ ìë™í™”**

```yaml
# .github/workflows/security-scan.yml
name: Security Vulnerability Scan
on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  schedule:
    - cron: '0 2 * * *'  # ë§¤ì¼ ìƒˆë²½ 2ì‹œ

jobs:
  vulnerability-scan:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'
      
      - name: Upload Trivy scan results to GitHub Security tab
        uses: github/codeql-action/upload-sarif@v2
        with:
          sarif_file: 'trivy-results.sarif'
```

#### ğŸ“Š **ë³´ì•ˆ ê°ì‚¬ ë¡œê·¸**

```python
class SecurityAuditLogger:
    def __init__(self):
        self.logger = structlog.get_logger("security_audit")
        
    def log_api_access(self, user_id: str, action: str, resource: str, ip_address: str):
        """
        API ì ‘ê·¼ ë¡œê·¸ ê¸°ë¡
        """
        self.logger.info(
            "api_access",
            user_id=user_id,
            action=action,
            resource=resource,
            ip_address=ip_address,
            timestamp=datetime.utcnow().isoformat(),
            severity="INFO"
        )
    
    def log_suspicious_activity(self, activity_type: str, details: dict, severity: str = "HIGH"):
        """
        ì˜ì‹¬ìŠ¤ëŸ¬ìš´ í™œë™ ë¡œê·¸
        """
        self.logger.warning(
            "suspicious_activity",
            activity_type=activity_type,
            details=details,
            severity=severity,
            timestamp=datetime.utcnow().isoformat()
        )
        
        # ì¦‰ì‹œ ì•Œë¦¼ ë°œì†¡
        if severity in ["HIGH", "CRITICAL"]:
            self.send_security_alert(activity_type, details)
```

#### ğŸ›¡ï¸ **ì¹¨ì… íƒì§€ ì‹œìŠ¤í…œ (IDS)**

```python
class IntrusionDetectionSystem:
    def __init__(self):
        self.anomaly_detector = AnomalyDetector()
        self.rate_limiter = RateLimiter()
        self.geo_filter = GeoLocationFilter()
        
    async def analyze_request(self, request_info: dict) -> dict:
        """
        ë“¤ì–´ì˜¤ëŠ” ìš”ì²­ ë¶„ì„
        """
        analysis_result = {
            'allowed': True,
            'risk_score': 0,
            'reasons': []
        }
        
        # 1. ì§€ë¦¬ì  ìœ„ì¹˜ ê²€ì‚¬
        if not self.geo_filter.is_allowed_location(request_info['ip_address']):
            analysis_result['allowed'] = False
            analysis_result['risk_score'] += 50
            analysis_result['reasons'].append('geo_location_blocked')
        
        # 2. ë¹„ì •ìƒì ì¸ ì ‘ê·¼ íŒ¨í„´ ê²€ì‚¬
        anomaly_score = await self.anomaly_detector.analyze_pattern(request_info)
        if anomaly_score > 0.8:
            analysis_result['allowed'] = False
            analysis_result['risk_score'] += 30
            analysis_result['reasons'].append('anomalous_pattern')
        
        # 3. ì†ë„ ì œí•œ ê²€ì‚¬
        if not self.rate_limiter.is_allowed(request_info['user_id']):
            analysis_result['allowed'] = False
            analysis_result['risk_score'] += 20
            analysis_result['reasons'].append('rate_limit_exceeded')
        
        return analysis_result
```

---

## ğŸ“Š 5. ì§€ëŠ¥í˜• ëª¨ë‹ˆí„°ë§, ë¡œê¹… ë° ì•Œë¦¼

### 5.1 ìš´ì˜ì˜ ì¤‘ìš”ì„±

**"ë¸”ë™ë°•ìŠ¤" ìš´ì˜ ë¶ˆê°€**: 24/7 ê¸ˆìœµ ì‹œìŠ¤í…œì—ì„œ ìš´ì˜ ìƒíƒœ í™•ì¸, ë””ë²„ê¹…, ì„±ëŠ¥ ë¶„ì„ì„ ìœ„í•œ í¬ê´„ì ì¸ ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹… ì „ëµì€ **ì ˆëŒ€ì ìœ¼ë¡œ í•„ìˆ˜ì **

### 5.2 ë¡œê¹… (Logging)

#### ğŸ“ **ë¡œê·¸ í˜•ì‹ ë° ì¶œë ¥**
- **í˜•ì‹**: ëª¨ë“  ì„œë¹„ìŠ¤ëŠ” **êµ¬ì¡°í™”ëœ ë¡œê·¸(JSON í˜•ì‹)** ì‚¬ìš©
- **ì¶œë ¥**: **stdout**ìœ¼ë¡œ ì¶œë ¥
- **ìˆ˜ì§‘**: GCEì— ê¸°ë³¸ ì„¤ì¹˜ëœ ë¡œê¹… ì—ì´ì „íŠ¸ê°€ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ì—¬ **Google Cloud Logging**ìœ¼ë¡œ ì „ì†¡

#### ğŸ¯ **íš¨ê³¼**
**ë‹¨ì¼ ì°½êµ¬**: ì‹œìŠ¤í…œ ì „ì²´ì˜ ëª¨ë“  ë¡œê·¸ë¥¼ ì¤‘ì•™ì—ì„œ ê²€ìƒ‰í•˜ê³  ë¶„ì„í•  ìˆ˜ ìˆëŠ” í†µí•© í™˜ê²½

### 5.3 ì§€ëŠ¥í˜• ëª¨ë‹ˆí„°ë§ (Enhanced Monitoring)

#### ğŸ“ˆ **í•µì‹¬ ì„±ê³¼ ì§€í‘œ(KPI) í™•ì¥**

ê° ì„œë¹„ìŠ¤ì˜ KPIë¥¼ **Google Cloud Monitoring**ìœ¼ë¡œ ì „ì†¡:

| ì„œë¹„ìŠ¤ | ê¸°ìˆ  ë©”íŠ¸ë¦­ | ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ | ì„¤ëª… |
|---------|------------|----------------|------|
| **Core Engine** | í™œì„± ì „ëµ ìˆ˜, ë©”ì‹œì§€ í ëŒ€ê¸° ë©”ì‹œì§€ ìˆ˜ | ì´ í¬íŠ¸í´ë¦¬ì˜¤ PnL, í™œì„± í¬ì§€ì…˜ ìˆ˜ | ì‹œìŠ¤í…œ ì „ë°˜ì  ìƒíƒœ ë° ìˆ˜ìµì„± |
| **Exchange Connector** | API í˜¸ì¶œ ì§€ì—° ì‹œê°„, ì†ë„ ì œí•œ ì‚¬ìš©ë¥  | ê±°ë˜ ì²´ê²°ë¥ , ìŠ¬ë¦¬í”¼ì§€ í‰ê·  | ì™¸ë¶€ ì—°ê²° ìƒíƒœ ë° ê±°ë˜ í’ˆì§ˆ |
| **Strategy Worker** | ì‹¤í–‰ëœ ê±°ë˜ ìˆ˜, ì˜¤ë¥˜ ë°œìƒë¥  | ì „ëµë³„ PnL, ìƒ¤í”„ ì§€ìˆ˜, ìµœëŒ€ ì†ì‹¤ë¥  | ê°œë³„ ì „ëµ ì„±ê³¼ |
| **Capital Manager** | í• ë‹¹ ìš”ì²­ ì²˜ë¦¬ ì‹œê°„, ê±°ë¶€ìœ¨ | ë¦¬ìŠ¤í¬ ì˜ˆì‚° ì‚¬ìš©ë¥ , ROI | ë¦¬ìŠ¤í¬ ê´€ë¦¬ íš¨ê³¼ì„± |

#### ğŸ¤– **AI ê¸°ë°˜ ì´ìƒ ê°ì§€**

```python
class AIAnomalyDetector:
    def __init__(self):
        self.model = self.load_anomaly_detection_model()
        self.feature_extractor = MetricsFeatureExtractor()
        self.threshold_manager = DynamicThresholdManager()
        
    async def detect_anomalies(self, metrics_data: dict) -> list:
        """
        AI ëª¨ë¸ì„ ì‚¬ìš©í•œ ì´ìƒ ê°ì§€
        """
        # íŠ¹ì„± ì¶”ì¶œ
        features = self.feature_extractor.extract_features(metrics_data)
        
        # ì´ìƒ ì ìˆ˜ ê³„ì‚°
        anomaly_scores = self.model.predict(features)
        
        # ë™ì  ì„ê³„ê°’ ì ìš©
        dynamic_threshold = self.threshold_manager.get_threshold(
            time_of_day=datetime.now().hour,
            day_of_week=datetime.now().weekday(),
            market_volatility=metrics_data.get('market_volatility', 0)
        )
        
        anomalies = []
        for i, score in enumerate(anomaly_scores):
            if score > dynamic_threshold:
                anomalies.append({
                    'metric_name': features[i]['name'],
                    'anomaly_score': score,
                    'severity': self.calculate_severity(score),
                    'predicted_impact': self.predict_impact(features[i])
                })
        
        return anomalies
    
    def predict_impact(self, feature_data: dict) -> dict:
        """
        ì´ìƒ ìƒí™©ì˜ ì˜ˆìƒ ì˜í–¥ë„ ì˜ˆì¸¡
        """
        impact_model = self.load_impact_prediction_model()
        
        predicted_impact = impact_model.predict([feature_data])
        
        return {
            'financial_impact': predicted_impact[0],
            'operational_impact': predicted_impact[1],
            'estimated_duration': predicted_impact[2],
            'confidence': predicted_impact[3]
        }
```

#### ğŸ“Š **ì„±ê³¼ ëŒ€ì‹œë³´ë“œ êµ¬ì„±**

```python
class BusinessMetricsDashboard:
    def __init__(self):
        self.grafana_client = GrafanaClient()
        self.data_aggregator = MetricsAggregator()
        
    def create_executive_dashboard(self):
        """
        ê²½ì˜ì§„ì„ ìœ„í•œ ê³ ìˆ˜ì¤€ ëŒ€ì‹œë³´ë“œ ìƒì„±
        """
        dashboard_config = {
            'title': 'Trading Bot Executive Dashboard',
            'panels': [
                {
                    'title': 'Total Portfolio Value',
                    'type': 'singlestat',
                    'targets': ['portfolio_total_value'],
                    'format': 'currency'
                },
                {
                    'title': 'Daily P&L',
                    'type': 'graph',
                    'targets': ['daily_pnl_by_strategy'],
                    'timeRange': '7d'
                },
                {
                    'title': 'Risk Metrics',
                    'type': 'table',
                    'targets': ['max_drawdown', 'var_95', 'sharpe_ratio'],
                    'thresholds': {
                        'max_drawdown': {'warning': 10, 'critical': 15},
                        'var_95': {'warning': 5, 'critical': 10}
                    }
                },
                {
                    'title': 'System Health',
                    'type': 'heatmap',
                    'targets': ['service_availability', 'error_rates'],
                    'colors': ['green', 'yellow', 'red']
                }
            ]
        }
        
        return self.grafana_client.create_dashboard(dashboard_config)
    
    async def generate_daily_report(self) -> dict:
        """
        ì¼ì¼ ì„±ê³¼ ë³´ê³ ì„œ ìë™ ìƒì„±
        """
        today = datetime.now().date()
        
        # ë°ì´í„° ìˆ˜ì§‘
        portfolio_metrics = await self.data_aggregator.get_portfolio_metrics(today)
        strategy_performance = await self.data_aggregator.get_strategy_performance(today)
        risk_metrics = await self.data_aggregator.get_risk_metrics(today)
        system_health = await self.data_aggregator.get_system_health(today)
        
        # ë³´ê³ ì„œ ìƒì„±
        report = {
            'date': today.isoformat(),
            'summary': {
                'total_pnl': portfolio_metrics['daily_pnl'],
                'best_strategy': strategy_performance['best_performer'],
                'worst_strategy': strategy_performance['worst_performer'],
                'system_uptime': system_health['uptime_percentage']
            },
            'detailed_analysis': {
                'portfolio': portfolio_metrics,
                'strategies': strategy_performance,
                'risk': risk_metrics,
                'technical': system_health
            },
            'recommendations': self.generate_recommendations(
                portfolio_metrics, strategy_performance, risk_metrics
            )
        }
        
        return report
```

### 5.4 ì˜ˆì¸¡ì  ì•Œë¦¼ ì‹œìŠ¤í…œ (Enhanced Alerting)

#### ğŸš¨ **ì§€ëŠ¥í˜• ì•Œë¦¼ êµ¬ì„±**

**ê¸°ì¡´ ì„ê³„ê°’ ê¸°ë°˜ ì•Œë¦¼ì˜ í•œê³„ ê·¹ë³µ**:

```python
class PredictiveAlertingSystem:
    def __init__(self):
        self.trend_analyzer = TrendAnalyzer()
        self.alert_fatigue_preventer = AlertFatiguePreventer()
        self.escalation_manager = EscalationManager()
        
    async def analyze_and_alert(self, metric_data: dict):
        """
        ì˜ˆì¸¡ì  ë¶„ì„ ê¸°ë°˜ ì•Œë¦¼ ì‹œìŠ¤í…œ
        """
        # 1. íŠ¸ë Œë“œ ë¶„ì„
        trend_analysis = await self.trend_analyzer.analyze_trends(metric_data)
        
        # 2. ë¯¸ë˜ ìƒíƒœ ì˜ˆì¸¡
        future_predictions = await self.predict_future_state(
            metric_data, horizon_minutes=30
        )
        
        # 3. ì ì¬ì  ë¬¸ì œ ì‹ë³„
        potential_issues = await self.identify_potential_issues(
            trend_analysis, future_predictions
        )
        
        # 4. ì•Œë¦¼ í•„ìš”ì„± íŒë‹¨
        for issue in potential_issues:
            if await self.should_alert(issue):
                await self.send_predictive_alert(issue)
    
    async def predict_future_state(self, current_data: dict, horizon_minutes: int) -> dict:
        """
        ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ì„ ì‚¬ìš©í•œ ë¯¸ë˜ ìƒíƒœ ì˜ˆì¸¡
        """
        predictions = {}
        
        for metric_name, values in current_data.items():
            # ARIMA ë˜ëŠ” Prophet ëª¨ë¸ ì‚¬ìš©
            model = self.get_prediction_model(metric_name)
            
            future_values = model.forecast(steps=horizon_minutes)
            confidence_intervals = model.forecast_confidence_intervals(steps=horizon_minutes)
            
            predictions[metric_name] = {
                'predicted_values': future_values,
                'confidence_lower': confidence_intervals['lower'],
                'confidence_upper': confidence_intervals['upper'],
                'trend_direction': self.analyze_trend_direction(future_values)
            }
        
        return predictions
    
    async def send_predictive_alert(self, issue: dict):
        """
        ì˜ˆì¸¡ì  ì•Œë¦¼ ë°œì†¡ (ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
        """
        alert_message = {
            'type': 'predictive_alert',
            'severity': issue['predicted_severity'],
            'title': f"Potential Issue Detected: {issue['metric_name']}",
            'description': issue['description'],
            'current_value': issue['current_value'],
            'predicted_value': issue['predicted_value'],
            'time_to_impact': issue['time_to_impact'],
            'recommended_actions': issue['recommended_actions'],
            'confidence': issue['prediction_confidence']
        }
        
        # ë‹¤ì¤‘ ì±„ë„ ì•Œë¦¼
        await self.send_telegram_alert(alert_message)
        await self.send_pagerduty_alert(alert_message)
        await self.log_alert_to_system(alert_message)
```

#### ğŸ“ˆ **ì„±ê³¼ ê¸°ë°˜ ì•Œë¦¼**

```python
class PerformanceBasedAlerting:
    def __init__(self):
        self.performance_tracker = PerformanceTracker()
        self.benchmark_comparator = BenchmarkComparator()
        
    async def monitor_performance_targets(self):
        """
        ì„±ê³¼ ëª©í‘œ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼
        """
        while True:
            # í˜„ì¬ ì„±ê³¼ í™•ì¸
            current_performance = await self.performance_tracker.get_current_metrics()
            
            # ëª©í‘œ ëŒ€ë¹„ ì„±ê³¼ ë¶„ì„
            performance_analysis = await self.analyze_vs_targets(current_performance)
            
            # ë²¤ì¹˜ë§ˆí¬ ëŒ€ë¹„ ì„±ê³¼ ë¶„ì„
            benchmark_analysis = await self.benchmark_comparator.compare(current_performance)
            
            # ì•Œë¦¼ ì¡°ê±´ ê²€ì‚¬
            alerts = []
            
            # ëª©í‘œ ìˆ˜ìµë¥  ë‹¬ì„±
            if performance_analysis['monthly_return'] >= performance_analysis['target_return']:
                alerts.append(self.create_success_alert('target_return_achieved', performance_analysis))
            
            # ì†ì‹¤ ì„ê³„ì  ê·¼ì ‘
            if current_performance['drawdown'] >= (performance_analysis['max_drawdown_limit'] * 0.8):
                alerts.append(self.create_warning_alert('drawdown_approaching_limit', current_performance))
            
            # ë²¤ì¹˜ë§ˆí¬ ëŒ€ë¹„ underperformance
            if benchmark_analysis['relative_performance'] < -0.05:  # 5% ì´ìƒ underperform
                alerts.append(self.create_performance_alert('underperforming_benchmark', benchmark_analysis))
            
            # ì•Œë¦¼ ë°œì†¡
            for alert in alerts:
                await self.send_performance_alert(alert)
            
            await asyncio.sleep(300)  # 5ë¶„ë§ˆë‹¤ ì²´í¬
```

### 5.5 ê¸´ê¸‰ ì•Œë¦¼ ì²´ê³„ (Crisis Management)

#### ğŸ”¥ **ë‹¤ë‹¨ê³„ ì—ìŠ¤ì»¬ë ˆì´ì…˜**

```python
class CrisisManagementSystem:
    def __init__(self):
        self.severity_levels = {
            'low': {'response_time': 3600, 'escalation_delay': 7200},      # 1ì‹œê°„, 2ì‹œê°„
            'medium': {'response_time': 1800, 'escalation_delay': 3600},   # 30ë¶„, 1ì‹œê°„
            'high': {'response_time': 300, 'escalation_delay': 900},       # 5ë¶„, 15ë¶„
            'critical': {'response_time': 60, 'escalation_delay': 300}     # 1ë¶„, 5ë¶„
        }
        
    async def handle_crisis_event(self, event: dict):
        """
        ìœ„ê¸° ìƒí™© ì²˜ë¦¬
        """
        severity = self.determine_severity(event)
        
        # ì¦‰ì‹œ ëŒ€ì‘ ì¡°ì¹˜
        immediate_actions = await self.execute_immediate_response(event, severity)
        
        # ì•Œë¦¼ ë°œì†¡
        await self.send_crisis_alert(event, severity, immediate_actions)
        
        # ì—ìŠ¤ì»¬ë ˆì´ì…˜ íƒ€ì´ë¨¸ ì‹œì‘
        asyncio.create_task(self.monitor_response_and_escalate(event, severity))
    
    async def execute_immediate_response(self, event: dict, severity: str) -> list:
        """
        ì¦‰ê°ì ì¸ ìë™ ëŒ€ì‘ ì¡°ì¹˜
        """
        actions_taken = []
        
        if event['type'] == 'liquidation_risk':
            # ëª¨ë“  ê±°ë˜ ì¼ì‹œ ì¤‘ë‹¨
            await self.emergency_stop_all_trading()
            actions_taken.append('emergency_trading_halt')
            
            # ë¦¬ìŠ¤í¬ í¬ì§€ì…˜ ìë™ ì²­ì‚°
            if severity == 'critical':
                await self.emergency_position_liquidation()
                actions_taken.append('emergency_liquidation')
        
        elif event['type'] == 'api_failure':
            # ë°±ì—… API í‚¤ë¡œ ì „í™˜
            await self.switch_to_backup_api()
            actions_taken.append('api_failover')
        
        elif event['type'] == 'database_failure':
            # ì½ê¸° ì „ìš© ëª¨ë“œë¡œ ì „í™˜
            await self.switch_to_readonly_mode()
            actions_taken.append('readonly_mode_activated')
        
        return actions_taken
```

---

## ğŸ”„ 6. ì¬í•´ ë³µêµ¬ ë° ê³ ê°€ìš©ì„± (ì‹ ê·œ ì¶”ê°€)

### 6.1 ì¬í•´ ë³µêµ¬ ì „ëµ ê°œìš”

**ëª©í‘œ**: RTO(Recovery Time Objective) < 15ë¶„, RPO(Recovery Point Objective) < 5ë¶„

**ë²”ìœ„**: 
- ğŸ”¥ ì™„ì „í•œ GCP ì§€ì—­ ì¥ì• 
- ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì†ìƒ ë˜ëŠ” ì‚­ì œ
- ğŸ–¥ï¸ ì»´í“¨íŒ… ì¸ìŠ¤í„´ìŠ¤ ì¥ì• 
- ğŸŒ ë„¤íŠ¸ì›Œí¬ ë¶„í•  ìƒí™©

### 6.2 ë°ì´í„° ë°±ì—… ì „ëµ

#### ğŸ“Š **ë‹¤ì¸µì  ë°±ì—… ì•„í‚¤í…ì²˜**

```yaml
# terraform/backup_strategy.tf
resource "google_sql_database_instance_backup_retention_policy" "main" {
  instance = google_sql_database_instance.trading_bot_db.name
  
  # ì¼ì¼ ë°±ì—…
  backup_retention_settings {
    retained_backups = 30
    retention_unit   = "COUNT"
  }
  
  # í¬ì¸íŠ¸-ì¸-íƒ€ì„ ë³µêµ¬
  point_in_time_recovery_enabled = true
  transaction_log_retention_days = 7
}

# êµì°¨ ì§€ì—­ ë°±ì—…
resource "google_sql_database_instance" "backup_replica" {
  name             = "trading-bot-backup-replica"
  database_version = "POSTGRES_15"
  region          = "us-east1"  # ì£¼ ì§€ì—­ê³¼ ë‹¤ë¥¸ ì§€ì—­
  
  replica_configuration {
    master_instance_name = google_sql_database_instance.trading_bot_db.name
    replica_type        = "READ_REPLICA"
  }
}
```

#### ğŸ’¾ **ì¤‘ìš” ë°ì´í„° ì¶”ê°€ ë°±ì—…**

```python
class DataBackupManager:
    def __init__(self):
        self.gcs_client = storage.Client()
        self.backup_bucket = 'trading-bot-critical-backups'
        
    async def backup_critical_configurations(self):
        """
        ì¤‘ìš” ì„¤ì • ë°ì´í„° ë°±ì—…
        """
        critical_data = {
            'strategies': await self.export_strategies(),
            'portfolios': await self.export_portfolios(),
            'portfolio_rules': await self.export_portfolio_rules(),
            'api_configurations': await self.export_api_configs()
        }
        
        # ì•”í˜¸í™” í›„ GCSì— ì €ì¥
        encrypted_data = self.encrypt_sensitive_data(critical_data)
        
        backup_filename = f"critical_backup_{datetime.now().isoformat()}.json.enc"
        
        await self.upload_to_gcs(
            bucket_name=self.backup_bucket,
            filename=backup_filename,
            data=encrypted_data
        )
        
        # ë°±ì—… ì„±ê³µ ë¡œê·¸
        logger.info(f"Critical data backup completed: {backup_filename}")
    
    async def schedule_automated_backups(self):
        """
        ìë™í™”ëœ ë°±ì—… ìŠ¤ì¼€ì¤„ë§
        """
        # ë§¤ì¼ ìƒˆë²½ 3ì‹œì— ì „ì²´ ë°±ì—…
        scheduler.add_job(
            func=self.backup_critical_configurations,
            trigger=CronTrigger(hour=3, minute=0),
            id='daily_backup',
            replace_existing=True
        )
        
        # ë§¤ ì‹œê°„ë§ˆë‹¤ ì¦ë¶„ ë°±ì—…
        scheduler.add_job(
            func=self.incremental_backup,
            trigger=CronTrigger(minute=0),
            id='hourly_incremental_backup',
            replace_existing=True
        )
```

### 6.3 ì¥ì•  ì¡°ì¹˜ (Failover) í”„ë¡œì„¸ìŠ¤

#### ğŸ”„ **ìë™ ì¥ì•  ì¡°ì¹˜**

```python
class AutoFailoverManager:
    def __init__(self):
        self.health_checker = HealthChecker()
        self.dns_manager = DNSManager()
        self.load_balancer = LoadBalancerManager()
        
    async def monitor_primary_system(self):
        """
        ì£¼ ì‹œìŠ¤í…œ ìƒíƒœ ì§€ì† ëª¨ë‹ˆí„°ë§
        """
        consecutive_failures = 0
        
        while True:
            try:
                # ì¢…í•© í—¬ìŠ¤ ì²´í¬
                health_status = await self.health_checker.comprehensive_check()
                
                if health_status['overall_health'] == 'healthy':
                    consecutive_failures = 0
                else:
                    consecutive_failures += 1
                    
                    # 3íšŒ ì—°ì† ì‹¤íŒ¨ ì‹œ ì¥ì•  ì¡°ì¹˜ ì‹œì‘
                    if consecutive_failures >= 3:
                        await self.initiate_failover()
                        break
                        
            except Exception as e:
                logger.error(f"Health check failed: {e}")
                consecutive_failures += 1
                
            await asyncio.sleep(30)  # 30ì´ˆë§ˆë‹¤ ì²´í¬
    
    async def initiate_failover(self):
        """
        ë°±ì—… ì‹œìŠ¤í…œìœ¼ë¡œ ì¥ì•  ì¡°ì¹˜ ì‹¤í–‰
        """
        logger.critical("Initiating failover to backup region")
        
        try:
            # 1. ë°±ì—… ì§€ì—­ì˜ ì¸ìŠ¤í„´ìŠ¤ í™œì„±í™”
            await self.activate_backup_instances()
            
            # 2. ë°ì´í„°ë² ì´ìŠ¤ ë³µì œë³¸ì„ ì£¼ ë°ì´í„°ë² ì´ìŠ¤ë¡œ ìŠ¹ê²©
            await self.promote_backup_database()
            
            # 3. DNS ë ˆì½”ë“œ ì—…ë°ì´íŠ¸ (íŠ¸ë˜í”½ ì „í™˜)
            await self.dns_manager.update_records_to_backup()
            
            # 4. ë¡œë“œ ë°¸ëŸ°ì„œ ì„¤ì • ë³€ê²½
            await self.load_balancer.redirect_to_backup()
            
            # 5. ë°±ì—… ì‹œìŠ¤í…œì—ì„œ ì„œë¹„ìŠ¤ ì‹œì‘
            await self.start_services_on_backup()
            
            # 6. ìƒíƒœ ì¡°ì • í”„ë¡œí† ì½œ ì‹¤í–‰
            await self.run_state_reconciliation()
            
            logger.info("Failover completed successfully")
            
            # ìš´ì˜íŒ€ì— ê¸´ê¸‰ ì•Œë¦¼
            await self.send_failover_notification()
            
        except Exception as e:
            logger.critical(f"Failover failed: {e}")
            await self.send_critical_failure_alert()
```

### 6.4 ë‹¤ì¤‘ ì§€ì—­ ë°°í¬ ì „ëµ

#### ğŸŒ **ì§€ì—­ë³„ ë°°í¬ ì•„í‚¤í…ì²˜**

```yaml
# terraform/multi_region_deployment.tf
# ì£¼ ì§€ì—­ (us-central1)
module "primary_region" {
  source = "./modules/trading_bot_region"
  
  region = "us-central1"
  is_primary = true
  
  database_config = {
    instance_type = "db-custom-4-16384"
    backup_enabled = true
    ha_enabled = true
  }
  
  compute_config = {
    instance_type = "e2-standard-4"
    auto_scaling = true
    min_instances = 2
    max_instances = 10
  }
}

# ë°±ì—… ì§€ì—­ (us-east1)
module "backup_region" {
  source = "./modules/trading_bot_region"
  
  region = "us-east1"
  is_primary = false
  
  database_config = {
    instance_type = "db-custom-2-8192"  # ë°±ì—…ì€ ë” ì‘ì€ ì¸ìŠ¤í„´ìŠ¤
    replica_of = module.primary_region.database_instance_name
  }
  
  compute_config = {
    instance_type = "e2-standard-2"
    auto_scaling = false
    min_instances = 1
    max_instances = 3
  }
}

# ê¸€ë¡œë²Œ ë¡œë“œ ë°¸ëŸ°ì„œ
resource "google_compute_global_forwarding_rule" "default" {
  name       = "trading-bot-global-lb"
  target     = google_compute_target_http_proxy.default.id
  port_range = "80"
}
```

### 6.5 ë³µêµ¬ í…ŒìŠ¤íŠ¸ ë° í›ˆë ¨

#### ğŸ§ª **ì •ê¸°ì ì¸ DR í…ŒìŠ¤íŠ¸**

```python
class DisasterRecoveryTester:
    def __init__(self):
        self.test_scheduler = TestScheduler()
        self.metrics_collector = DRMetricsCollector()
        
    async def monthly_dr_test(self):
        """
        ì›”ë³„ ì¬í•´ ë³µêµ¬ í…ŒìŠ¤íŠ¸
        """
        test_start_time = datetime.now()
        
        try:
            # 1. ëª¨ì˜ ì¥ì•  ìƒí™© ìƒì„±
            await self.simulate_primary_failure()
            
            # 2. ìë™ ì¥ì•  ì¡°ì¹˜ í…ŒìŠ¤íŠ¸
            failover_start = datetime.now()
            await self.trigger_automated_failover()
            failover_end = datetime.now()
            
            # 3. ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦
            data_integrity_check = await self.verify_data_integrity()
            
            # 4. ì„œë¹„ìŠ¤ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
            service_functionality = await self.test_service_functionality()
            
            # 5. ë³µêµ¬ ì‹œê°„ ì¸¡ì •
            rto_actual = (failover_end - failover_start).total_seconds()
            
            # 6. í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê¸°ë¡
            test_results = {
                'test_date': test_start_time.isoformat(),
                'rto_actual': rto_actual,
                'rto_target': 900,  # 15ë¶„
                'data_integrity': data_integrity_check,
                'service_functionality': service_functionality,
                'success': rto_actual <= 900 and data_integrity_check and service_functionality
            }
            
            await self.record_test_results(test_results)
            
            # 7. ì›ë˜ ìƒíƒœë¡œ ë³µêµ¬
            await self.restore_primary_system()
            
        except Exception as e:
            logger.error(f"DR test failed: {e}")
            await self.send_dr_test_failure_alert()
```

---

## ğŸš€ 7. ì»¨í…Œì´ë„ˆ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ (ì‹ ê·œ ì¶”ê°€)

### 7.1 Kubernetes ë°°í¬ ì˜µì…˜

#### âš™ï¸ **GKE ê¸°ë°˜ í˜„ëŒ€í™” ë°°í¬**

**ê¸°ì¡´ GCE + systemd vs ìƒˆë¡œìš´ GKE ì˜µì…˜ ë¹„êµ**:

| ì¸¡ë©´ | GCE + systemd | GKE (Kubernetes) |
|------|---------------|------------------|
| **ê´€ë¦¬ ë³µì¡ì„±** | ë‚®ìŒ | ì¤‘ê°„ |
| **í™•ì¥ì„±** | ìˆ˜ë™ | ìë™ |
| **ê³ ê°€ìš©ì„±** | ì œí•œì  | ë„¤ì´í‹°ë¸Œ ì§€ì› |
| **ë¡¤ë§ ì—…ë°ì´íŠ¸** | ìˆ˜ë™ êµ¬í˜„ | ë‚´ì¥ ê¸°ëŠ¥ |
| **ì„œë¹„ìŠ¤ ë””ìŠ¤ì»¤ë²„ë¦¬** | ìˆ˜ë™ êµ¬ì„± | ìë™ |
| **ë¹„ìš©** | ë‚®ìŒ | ì¤‘ê°„ |

#### ğŸ”§ **Kubernetes ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì˜ˆì‹œ**

```yaml
# k8s/trading-bot-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: core-engine
  namespace: trading-bot
spec:
  replicas: 2
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  selector:
    matchLabels:
      app: core-engine
  template:
    metadata:
      labels:
        app: core-engine
    spec:
      serviceAccountName: trading-bot-sa
      containers:
      - name: core-engine
        image: us-central1-docker.pkg.dev/PROJECT_ID/trading-bot-repo/core-engine:latest
        ports:
        - containerPort: 8080
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: database-credentials
              key: url
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: core-engine-service
  namespace: trading-bot
spec:
  selector:
    app: core-engine
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8080
  type: ClusterIP
```

### 7.2 Helm ì°¨íŠ¸ êµ¬ì„±

#### ğŸ“¦ **íŒ¨í‚¤ì§€ ê´€ë¦¬ ë° í™˜ê²½ë³„ ë°°í¬**

```yaml
# helm/trading-bot/values.yaml
global:
  image:
    registry: us-central1-docker.pkg.dev
    repository: PROJECT_ID/trading-bot-repo
    tag: "latest"
  
environment: production

coreEngine:
  enabled: true
  replicaCount: 2
  image:
    name: core-engine
  resources:
    requests:
      memory: 512Mi
      cpu: 500m
    limits:
      memory: 1Gi
      cpu: 1000m

strategyWorkers:
  enabled: true
  workers:
    - name: ma-crossover
      replicaCount: 1
      strategyId: "123"
    - name: grid-trader
      replicaCount: 1
      strategyId: "456"

exchangeConnector:
  enabled: true
  replicaCount: 2
  rateLimiting:
    enabled: true
    requestsPerSecond: 10

capitalManager:
  enabled: true
  replicaCount: 1  # ìƒíƒœ ì €ì¥ ì„œë¹„ìŠ¤ë¡œ ë‹¨ì¼ ì¸ìŠ¤í„´ìŠ¤

rabbitmq:
  enabled: true
  persistence:
    enabled: true
    size: 10Gi
  clustering:
    enabled: true
    replicaCount: 3

postgresql:
  enabled: false  # ì™¸ë¶€ Cloud SQL ì‚¬ìš©
  
monitoring:
  prometheus:
    enabled: true
  grafana:
    enabled: true
  alertmanager:
    enabled: true
```

### 7.3 ì„œë¹„ìŠ¤ ë©”ì‹œ í†µí•© (Istio)

#### ğŸŒ **ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ê°„ í†µì‹  ë³´ì•ˆ ë° ê´€ì°°ì„±**

```yaml
# istio/virtual-service.yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: trading-bot-vs
  namespace: trading-bot
spec:
  hosts:
  - trading-bot.example.com
  gateways:
  - trading-bot-gateway
  http:
  - match:
    - uri:
        prefix: /api/core
    route:
    - destination:
        host: core-engine-service
        port:
          number: 80
    fault:
      delay:
        percentage:
          value: 0.1
        fixedDelay: 5s  # ì¥ì•  ì‹œë®¬ë ˆì´ì…˜
    retries:
      attempts: 3
      perTryTimeout: 2s
---
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: trading-bot-dr
  namespace: trading-bot
spec:
  host: "*.trading-bot.svc.cluster.local"
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL  # mTLS ê°•ì œ
    connectionPool:
      tcp:
        maxConnections: 100
      http:
        http1MaxPendingRequests: 50
        maxRequestsPerConnection: 2
    circuitBreaker:
      consecutiveErrors: 3
      interval: 30s
      baseEjectionTime: 30s
```

---

## ğŸ¯ ì‹œìŠ¤í…œ í†µí•© íš¨ê³¼

### DevOps ì„±ìˆ™ë„ ë‹¬ì„±
- âœ… **ì™„ì „ ìë™í™”**: ìˆ˜ë™ ê°œì… ì—†ëŠ” ë°°í¬ íŒŒì´í”„ë¼ì¸
- âœ… **ì½”ë“œí˜• ì¸í”„ë¼**: ëª¨ë“  ì¸í”„ë¼ì˜ ë²„ì „ ê´€ë¦¬ ë° ê°ì‚¬ ê°€ëŠ¥
- âœ… **ë‹¤ì¸µì  ë³´ì•ˆ**: ë„¤íŠ¸ì›Œí¬ë¶€í„° ì• í”Œë¦¬ì¼€ì´ì…˜ê¹Œì§€ í¬ê´„ì  ë³´ì•ˆ
- âœ… **ì§€ëŠ¥í˜• ëª¨ë‹ˆí„°ë§**: AI ê¸°ë°˜ ì˜ˆì¸¡ì  ì•Œë¦¼ ë° ì´ìƒ ê°ì§€

### ìš´ì˜ ì•ˆì •ì„±
- âœ… **24/7 ë¬´ì¤‘ë‹¨**: systemd ê¸°ë°˜ ìë™ ë³µêµ¬ ì‹œìŠ¤í…œ
- âœ… **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: í¬ê´„ì ì¸ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ì•Œë¦¼
- âœ… **ì‹ ì†í•œ ëŒ€ì‘**: ìë™í™”ëœ ì¥ì•  ê°ì§€ ë° ì•Œë¦¼
- âœ… **ì¬í•´ ë³µêµ¬**: 15ë¶„ ì´ë‚´ ì™„ì „ ë³µêµ¬ ë³´ì¥

### ë³´ì•ˆ ê°•í™”
- âœ… **ìµœì†Œ ê¶Œí•œ**: IAM ê¸°ë°˜ ì„¸ë¶„í™”ëœ ê¶Œí•œ ê´€ë¦¬
- âœ… **ë¹„ë°€ ë³´í˜¸**: Secret Manager ê¸°ë°˜ ì¤‘ì•™ ì§‘ì¤‘ì‹ ë¹„ë°€ ê´€ë¦¬
- âœ… **ë„¤íŠ¸ì›Œí¬ ê²©ë¦¬**: VPC ë° ë°©í™”ë²½ ê¸°ë°˜ ë„¤íŠ¸ì›Œí¬ ë³´ì•ˆ
- âœ… **ì·¨ì•½ì  ê´€ë¦¬**: ìë™í™”ëœ ìŠ¤ìºë‹ ë° íŒ¨ì¹˜ ê´€ë¦¬

### ê³ ê¸‰ ê¸°ëŠ¥ ì¶”ê°€ íš¨ê³¼
- **ì˜ˆì¸¡ì  ìš´ì˜**: AI ê¸°ë°˜ ì¥ì•  ì˜ˆë°© ë° ì„±ëŠ¥ ìµœì í™”
- **ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸**: ê¸°ìˆ  ë©”íŠ¸ë¦­ì„ ë„˜ì–´ì„  ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ ì¸¡ì •
- **ìë™í™”ëœ ë³µêµ¬**: ì¸ê°„ ê°œì… ì—†ëŠ” ì¥ì•  ìƒí™© ìë™ ëŒ€ì‘
- **ë©€í‹° í´ë¼ìš°ë“œ ì¤€ë¹„**: Kubernetes ê¸°ë°˜ í”Œë«í¼ ë…ë¦½ì„±

### í™•ì¥ì„± ë° ë¯¸ë˜ ëŒ€ë¹„
- **ì»¨í…Œì´ë„ˆ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜**: Kubernetes ê¸°ë°˜ í˜„ëŒ€ì  ë°°í¬
- **ì„œë¹„ìŠ¤ ë©”ì‹œ**: Istioë¥¼ í†µí•œ ê³ ê¸‰ íŠ¸ë˜í”½ ê´€ë¦¬
- **ì˜µì €ë²„ë¹Œë¦¬í‹°**: ì™„ì „í•œ ë¶„ì‚° ì¶”ì  ë° ëª¨ë‹ˆí„°ë§
- **í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ**: ìµœì‹  í´ë¼ìš°ë“œ ê¸°ìˆ  ìŠ¤íƒ í™œìš©

---

## ğŸ“Š ìš´ì˜ ì§€í‘œ ë° SLA

### ì‹œìŠ¤í…œ ê°€ìš©ì„± ëª©í‘œ
- **ê°€ë™ ì‹œê°„**: 99.9% (ì›” 43ë¶„ ì´ë‚´ ë‹¤ìš´íƒ€ì„)
- **ì‘ë‹µ ì‹œê°„**: API í˜¸ì¶œ 95th percentile < 200ms
- **ì²˜ë¦¬ëŸ‰**: 10,000+ ê±°ë˜/ì‹œê°„ ì²˜ë¦¬ ëŠ¥ë ¥
- **ë³µêµ¬ ì‹œê°„**: ìë™ ì¥ì•  ì¡°ì¹˜ < 15ë¶„

### ë³´ì•ˆ ë° ê·œì • ì¤€ìˆ˜
- **ì·¨ì•½ì  ìŠ¤ìº”**: ì¼ 1íšŒ ìë™ ì‹¤í–‰
- **ë³´ì•ˆ íŒ¨ì¹˜**: ë°œê²¬ í›„ 72ì‹œê°„ ì´ë‚´ ì ìš©
- **ê°ì‚¬ ë¡œê·¸**: ëª¨ë“  ì¤‘ìš” ì‘ì—… 100% ê¸°ë¡
- **ì ‘ê·¼ ì œì–´**: ìµœì†Œ ê¶Œí•œ ì›ì¹™ 100% ì ìš©

### ë¹„ì¦ˆë‹ˆìŠ¤ ì—°ì†ì„±
- **ë°ì´í„° ì†ì‹¤**: RPO < 5ë¶„
- **ì„œë¹„ìŠ¤ ë³µêµ¬**: RTO < 15ë¶„
- **ë°±ì—… ì„±ê³µë¥ **: 99.95%
- **DR í…ŒìŠ¤íŠ¸**: ì›” 1íšŒ ì„±ê³µë¥  100%

---

## ğŸ“ ë¬¸ì„œ ê´€ë¦¬ ì •ë³´

**ì—°ê´€ ë¬¸ì„œ**: 
- `00_System_Overview_and_Architecture.md`
- `01_Core_Services_and_Execution_Framework.md`
- `05_Data_and_State_Management.md`

**í•µì‹¬ ê¸°ìˆ **: GCP, Docker, systemd, Terraform, Cloud Build, Kubernetes, Istio

**ë³´ì•ˆ ìš”êµ¬ì‚¬í•­**: ìµœì†Œ ê¶Œí•œ ì›ì¹™, Secret Manager, IP í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸, ì·¨ì•½ì  ìŠ¤ìºë‹

**êµ¬í˜„ ìš°ì„ ìˆœìœ„**: 
1. IaC ìŠ¤í¬ë¦½íŠ¸ ì‘ì„± (Terraform)
2. CI/CD íŒŒì´í”„ë¼ì¸ êµ¬ì¶• (Cloud Build)
3. ê¸°ë³¸ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ êµ¬ì„±
4. ì¬í•´ ë³µêµ¬ ê³„íš ìˆ˜ë¦½ ë° í…ŒìŠ¤íŠ¸
5. AI ê¸°ë°˜ ê³ ê¸‰ ëª¨ë‹ˆí„°ë§ êµ¬í˜„
6. Kubernetes ë§ˆì´ê·¸ë ˆì´ì…˜ (ì„ íƒì )

**ìš´ì˜ ì„±ìˆ™ë„**: Level 4 (ìµœì í™”) - ìë™í™”, ì˜ˆì¸¡, ì§€ì†ì  ê°œì„ 