#!/usr/bin/env python3
"""
24ì‹œê°„ ì—°ì† ë“œë¼ì´ëŸ° í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ
24-Hour Continuous Dry-Run Testing System

ì‹œìŠ¤í…œ ì•ˆì •ì„±ì„ ê²€ì¦í•˜ê¸° ìœ„í•œ 24ì‹œê°„ ì—°ì† í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬
- ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
- ìë™ ì˜¤ë¥˜ ê°ì§€ ë° ë³µêµ¬
- ìƒì„¸í•œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
- ìµœì í™”ëœ ì„±ëŠ¥ ê²€ì¦
"""

import asyncio
import json
import logging
import os
import sys
import time
from datetime import datetime, timezone, timedelta
from pathlib import Path
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any
import signal

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler("continuous_test.log", mode="a")
    ]
)
logger = logging.getLogger(__name__)


@dataclass
class TestMetrics:
    """í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­ ë°ì´í„° êµ¬ì¡°"""
    start_time: datetime
    end_time: Optional[datetime] = None
    total_operations: int = 0
    successful_operations: int = 0
    failed_operations: int = 0
    average_latency_ms: float = 0.0
    peak_latency_ms: float = 0.0
    min_latency_ms: float = float('inf')
    memory_usage_mb: float = 0.0
    cpu_usage_percent: float = 0.0
    errors: List[Dict[str, Any]] = field(default_factory=list)
    performance_samples: List[float] = field(default_factory=list)
    
    @property
    def success_rate(self) -> float:
        if self.total_operations == 0:
            return 0.0
        return (self.successful_operations / self.total_operations) * 100
    
    @property
    def duration_hours(self) -> float:
        if not self.end_time:
            end_time = datetime.now(timezone.utc)
        else:
            end_time = self.end_time
        return (end_time - self.start_time).total_seconds() / 3600
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "start_time": self.start_time.isoformat(),
            "end_time": self.end_time.isoformat() if self.end_time else None,
            "duration_hours": self.duration_hours,
            "total_operations": self.total_operations,
            "successful_operations": self.successful_operations,
            "failed_operations": self.failed_operations,
            "success_rate": self.success_rate,
            "average_latency_ms": self.average_latency_ms,
            "peak_latency_ms": self.peak_latency_ms,
            "min_latency_ms": self.min_latency_ms if self.min_latency_ms != float('inf') else 0.0,
            "memory_usage_mb": self.memory_usage_mb,
            "cpu_usage_percent": self.cpu_usage_percent,
            "error_count": len(self.errors),
            "latest_errors": self.errors[-5:] if self.errors else []
        }


class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.metrics = TestMetrics(start_time=datetime.now(timezone.utc))
        self.running = False
        
    async def record_operation(self, success: bool, latency_ms: float, error: Optional[str] = None):
        """ê°œë³„ ì—°ì‚° ê¸°ë¡"""
        self.metrics.total_operations += 1
        
        if success:
            self.metrics.successful_operations += 1
        else:
            self.metrics.failed_operations += 1
            if error:
                self.metrics.errors.append({
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "error": error,
                    "latency_ms": latency_ms
                })
        
        # ë ˆì´í„´ì‹œ í†µê³„ ì—…ë°ì´íŠ¸
        self.metrics.performance_samples.append(latency_ms)
        self.metrics.peak_latency_ms = max(self.metrics.peak_latency_ms, latency_ms)
        self.metrics.min_latency_ms = min(self.metrics.min_latency_ms, latency_ms)
        
        # í‰ê·  ë ˆì´í„´ì‹œ ê³„ì‚° (ìµœê·¼ 1000ê°œ ìƒ˜í”Œ)
        if len(self.metrics.performance_samples) > 1000:
            self.metrics.performance_samples = self.metrics.performance_samples[-1000:]
        
        if self.metrics.performance_samples:
            self.metrics.average_latency_ms = sum(self.metrics.performance_samples) / len(self.metrics.performance_samples)
    
    def get_current_metrics(self) -> Dict[str, Any]:
        """í˜„ì¬ ë©”íŠ¸ë¦­ ë°˜í™˜"""
        return self.metrics.to_dict()
    
    def save_metrics_report(self, filename: str):
        """ë©”íŠ¸ë¦­ ë³´ê³ ì„œ ì €ì¥"""
        report = self.get_current_metrics()
        with open(filename, 'w') as f:
            json.dump(report, f, indent=2)
        logger.info(f"ğŸ“Š Metrics report saved: {filename}")


class MockExchangeConnector:
    """í…ŒìŠ¤íŠ¸ìš© Mock Exchange Connector"""
    
    def __init__(self):
        self.connected = False
        self.base_latency = 0.5  # 0.5ms base latency
        self.error_rate = 0.001  # 0.1% error rate
        
    async def connect(self) -> bool:
        """ì—°ê²° ì‹œë®¬ë ˆì´ì…˜"""
        await asyncio.sleep(0.001)  # 1ms ì—°ê²° ì‹œê°„
        self.connected = True
        return True
    
    async def disconnect(self) -> bool:
        """ì—°ê²° í•´ì œ"""
        self.connected = False
        return True
    
    async def get_market_data(self, symbol: str, timeframe: str = "1m", limit: int = 50) -> List[Dict]:
        """ì‹œì¥ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì‹œë®¬ë ˆì´ì…˜"""
        if not self.connected:
            raise Exception("Not connected")
        
        # ì„±ëŠ¥ ì‹œë®¬ë ˆì´ì…˜ (0.5ms ~ 2ms ë²”ìœ„)
        latency = self.base_latency + (time.time() % 1.5)
        await asyncio.sleep(latency / 1000)
        
        # ë“œë¬¼ê²Œ ì˜¤ë¥˜ ë°œìƒ ì‹œë®¬ë ˆì´ì…˜
        if time.time() % 1000 < self.error_rate * 1000:
            raise Exception("Simulated network error")
        
        # Mock ë°ì´í„° ë°˜í™˜
        return [
            {
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "open": 50000.0,
                "high": 50100.0,
                "low": 49900.0,
                "close": 50050.0,
                "volume": 1000.0
            }
            for _ in range(limit)
        ]
    
    async def health_check(self) -> Dict[str, Any]:
        """í—¬ìŠ¤ì²´í¬"""
        return {
            "connected": self.connected,
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "status": "healthy" if self.connected else "disconnected"
        }


class ContinuousTestingSystem:
    """24ì‹œê°„ ì—°ì† í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ"""
    
    def __init__(self, test_duration_hours: float = 24.0):
        self.test_duration_hours = test_duration_hours
        self.monitor = PerformanceMonitor()
        self.exchange_connector = MockExchangeConnector()
        self.running = False
        self.stop_event = asyncio.Event()
        
        # í…ŒìŠ¤íŠ¸ ì„¤ì •
        self.operation_interval = 1.0  # 1ì´ˆë§ˆë‹¤ í…ŒìŠ¤íŠ¸ ì—°ì‚°
        self.health_check_interval = 60.0  # 1ë¶„ë§ˆë‹¤ í—¬ìŠ¤ì²´í¬
        self.report_interval = 300.0  # 5ë¶„ë§ˆë‹¤ ì§„í–‰ ë³´ê³ 
        
        # ì‹ í˜¸ í•¸ë“¤ëŸ¬ ì„¤ì •
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
    
    def _signal_handler(self, signum, frame):
        """ì¢…ë£Œ ì‹ í˜¸ ì²˜ë¦¬"""
        logger.info(f"Received signal {signum}, initiating graceful shutdown...")
        self.stop_event.set()
    
    async def start_test(self):
        """ì—°ì† í…ŒìŠ¤íŠ¸ ì‹œì‘"""
        logger.info("ğŸš€ Starting 24-Hour Continuous Testing System")
        logger.info(f"ğŸ“Š Test Duration: {self.test_duration_hours} hours")
        logger.info(f"âš¡ Target Performance: <1ms latency")
        logger.info("=" * 60)
        
        self.running = True
        start_time = datetime.now(timezone.utc)
        end_time = start_time + timedelta(hours=self.test_duration_hours)
        
        # Exchange connector ì—°ê²°
        connected = await self.exchange_connector.connect()
        if not connected:
            logger.error("âŒ Failed to connect to exchange connector")
            return False
        
        logger.info("âœ… Exchange connector connected")
        
        # ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ ì‹œì‘
        tasks = [
            asyncio.create_task(self._continuous_operations()),
            asyncio.create_task(self._health_check_loop()),
            asyncio.create_task(self._progress_reporting_loop()),
            asyncio.create_task(self._duration_monitor(end_time))
        ]
        
        try:
            # ëª¨ë“  íƒœìŠ¤í¬ ì‹¤í–‰
            await asyncio.gather(*tasks, return_exceptions=True)
            
        except Exception as e:
            logger.error(f"âŒ Test execution error: {e}")
            
        finally:
            # ì •ë¦¬ ì‘ì—…
            await self._cleanup()
            
        # ìµœì¢… ë³´ê³ ì„œ ìƒì„±
        await self._generate_final_report()
        
        return True
    
    async def _continuous_operations(self):
        """ì—°ì† ì—°ì‚° ìˆ˜í–‰"""
        logger.info("ğŸ”„ Starting continuous operations...")
        
        while self.running and not self.stop_event.is_set():
            try:
                # ì‹œì¥ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° í…ŒìŠ¤íŠ¸
                start_time = time.time()
                
                market_data = await self.exchange_connector.get_market_data("BTC/USDT")
                
                latency_ms = (time.time() - start_time) * 1000
                await self.monitor.record_operation(True, latency_ms)
                
                # ì„±ëŠ¥ ëª©í‘œ í™•ì¸ (1ms ëª©í‘œ)
                if latency_ms > 10.0:  # 10ms ì´ìƒì´ë©´ ê²½ê³ 
                    logger.warning(f"âš ï¸ High latency detected: {latency_ms:.2f}ms")
                
            except Exception as e:
                latency_ms = (time.time() - start_time) * 1000
                await self.monitor.record_operation(False, latency_ms, str(e))
                logger.error(f"âŒ Operation failed: {e}")
            
            # ë‹¤ìŒ ì—°ì‚°ê¹Œì§€ ëŒ€ê¸°
            await asyncio.sleep(self.operation_interval)
    
    async def _health_check_loop(self):
        """í—¬ìŠ¤ì²´í¬ ë£¨í”„"""
        while self.running and not self.stop_event.is_set():
            try:
                health = await self.exchange_connector.health_check()
                
                if not health.get("connected", False):
                    logger.error("âŒ Health check failed: connector not connected")
                    # ì¬ì—°ê²° ì‹œë„
                    await self.exchange_connector.connect()
                
            except Exception as e:
                logger.error(f"âŒ Health check error: {e}")
            
            await asyncio.sleep(self.health_check_interval)
    
    async def _progress_reporting_loop(self):
        """ì§„í–‰ ìƒí™© ë³´ê³  ë£¨í”„"""
        while self.running and not self.stop_event.is_set():
            metrics = self.monitor.get_current_metrics()
            
            logger.info("ğŸ“Š Progress Report")
            logger.info("-" * 40)
            logger.info(f"â±ï¸  Duration: {metrics['duration_hours']:.2f} hours")
            logger.info(f"ğŸ”¢ Total Operations: {metrics['total_operations']:,}")
            logger.info(f"âœ… Success Rate: {metrics['success_rate']:.2f}%")
            logger.info(f"âš¡ Avg Latency: {metrics['average_latency_ms']:.3f}ms")
            logger.info(f"ğŸ”¥ Peak Latency: {metrics['peak_latency_ms']:.3f}ms")
            logger.info(f"âš ï¸  Errors: {metrics['error_count']}")
            
            # ì„±ëŠ¥ í‰ê°€
            if metrics['average_latency_ms'] < 1.0:
                logger.info("ğŸ‰ EXCELLENT performance maintained")
            elif metrics['average_latency_ms'] < 5.0:
                logger.info("ğŸ¯ GOOD performance maintained") 
            elif metrics['average_latency_ms'] < 200.0:
                logger.info("âœ… ACCEPTABLE performance")
            else:
                logger.warning("âš ï¸ Performance degradation detected")
            
            logger.info("")
            
            await asyncio.sleep(self.report_interval)
    
    async def _duration_monitor(self, end_time: datetime):
        """í…ŒìŠ¤íŠ¸ ê¸°ê°„ ëª¨ë‹ˆí„°ë§"""
        while datetime.now(timezone.utc) < end_time and not self.stop_event.is_set():
            remaining = end_time - datetime.now(timezone.utc)
            
            if remaining.total_seconds() <= 0:
                logger.info("â° Test duration completed")
                self.stop_event.set()
                break
            
            await asyncio.sleep(60)  # 1ë¶„ë§ˆë‹¤ ì²´í¬
    
    async def _cleanup(self):
        """ì •ë¦¬ ì‘ì—…"""
        logger.info("ğŸ”„ Performing cleanup...")
        
        self.running = False
        self.monitor.metrics.end_time = datetime.now(timezone.utc)
        
        try:
            await self.exchange_connector.disconnect()
            logger.info("âœ… Exchange connector disconnected")
        except Exception as e:
            logger.error(f"âŒ Cleanup error: {e}")
    
    async def _generate_final_report(self):
        """ìµœì¢… ë³´ê³ ì„œ ìƒì„±"""
        logger.info("ğŸ“‹ Generating final test report...")
        
        metrics = self.monitor.get_current_metrics()
        
        # ì½˜ì†” ë³´ê³ ì„œ
        logger.info("ğŸ¯ 24-Hour Continuous Test Final Report")
        logger.info("=" * 60)
        logger.info(f"â±ï¸  Test Duration: {metrics['duration_hours']:.2f} hours")
        logger.info(f"ğŸ”¢ Total Operations: {metrics['total_operations']:,}")
        logger.info(f"âœ… Successful Operations: {metrics['successful_operations']:,}")
        logger.info(f"âŒ Failed Operations: {metrics['failed_operations']:,}")
        logger.info(f"ğŸ“Š Success Rate: {metrics['success_rate']:.2f}%")
        logger.info(f"âš¡ Average Latency: {metrics['average_latency_ms']:.3f}ms")
        logger.info(f"ğŸ”¥ Peak Latency: {metrics['peak_latency_ms']:.3f}ms")
        logger.info(f"ğŸš€ Min Latency: {metrics['min_latency_ms']:.3f}ms")
        logger.info(f"âš ï¸  Total Errors: {metrics['error_count']}")
        
        # ìµœì¢… ì„±ëŠ¥ í‰ê°€
        if metrics['success_rate'] >= 99.9 and metrics['average_latency_ms'] < 1.0:
            logger.info("ğŸ‰ EXCELLENT: Production-ready performance achieved!")
            grade = "A+"
        elif metrics['success_rate'] >= 99.5 and metrics['average_latency_ms'] < 5.0:
            logger.info("ğŸ¯ VERY GOOD: High-performance trading ready")
            grade = "A"
        elif metrics['success_rate'] >= 99.0 and metrics['average_latency_ms'] < 200.0:
            logger.info("âœ… GOOD: MVP requirements met")
            grade = "B"
        else:
            logger.warning("âš ï¸ NEEDS IMPROVEMENT: Performance issues detected")
            grade = "C"
        
        # íŒŒì¼ ë³´ê³ ì„œ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_filename = f"24h_test_report_{timestamp}.json"
        
        detailed_report = metrics.copy()
        detailed_report.update({
            "test_type": "24_hour_continuous_dry_run",
            "performance_grade": grade,
            "target_latency_ms": 1.0,
            "target_success_rate": 99.9,
            "targets_met": {
                "latency": metrics['average_latency_ms'] < 1.0,
                "success_rate": metrics['success_rate'] >= 99.9,
                "overall": grade in ["A+", "A"]
            }
        })
        
        with open(report_filename, 'w') as f:
            json.dump(detailed_report, f, indent=2)
        
        logger.info(f"ğŸ“„ Detailed report saved: {report_filename}")
        logger.info("=" * 60)
        
        return grade in ["A+", "A", "B"]


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # í…ŒìŠ¤íŠ¸ ê¸°ê°„ ì„¤ì • (ê¸°ë³¸ 24ì‹œê°„, í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì§§ê²Œ ì„¤ì • ê°€ëŠ¥)
    test_duration = float(os.getenv("TEST_DURATION_HOURS", "24.0"))
    
    # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 5ë¶„ìœ¼ë¡œ ì„¤ì • (ì‹¤ì œë¡œëŠ” 24ì‹œê°„)
    if len(sys.argv) > 1 and sys.argv[1] == "--quick":
        test_duration = 0.083  # 5ë¶„
        logger.info("ğŸš€ Quick test mode: 5 minutes")
    
    # ì—°ì† í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ ìƒì„± ë° ì‹¤í–‰
    test_system = ContinuousTestingSystem(test_duration_hours=test_duration)
    
    success = await test_system.start_test()
    
    if success:
        logger.info("ğŸ‰ Continuous testing completed successfully")
        return 0
    else:
        logger.error("âŒ Continuous testing failed")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)